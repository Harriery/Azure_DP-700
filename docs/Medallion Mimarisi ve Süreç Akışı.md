# ğŸ” Medallion Mimarisi ve SÃ¼reÃ§ AkÄ±ÅŸÄ± (Genel BakÄ±ÅŸ)
Microsoft Fabric + Medallion mimarisi ile Ã§alÄ±ÅŸÄ±rken:

## Bronze Katman (Ham veri)
Kaynaktan alÄ±nan ham veriler burada tutulur. Minimum iÅŸlem yapÄ±lÄ±r. Delta formatta tutulur.
â¤ KullanÄ±m amacÄ±: Veriyi deÄŸiÅŸtirmeden arÅŸivlemek, izlenebilirlik saÄŸlamak.

## Silver Katman (TemizlenmiÅŸ veri)
Bronze verilerden gelen veriler temizlenir, dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r (Ã¶rneÄŸin tarih formatÄ± dÃ¼zeltilir).
â¤ KullanÄ±m amacÄ±: Analiz edilebilir duruma getirmek.

## Gold Katman (Raporlama katmanÄ±)
Silver'dan gelen veriyle Ã¶lÃ§Ã¼m tablolarÄ± (fact) ve boyut tablolarÄ± (dimension) hazÄ±rlanÄ±r.
â¤ KullanÄ±m amacÄ±: Power BI veya baÅŸka BI araÃ§larÄ±nda doÄŸrudan kullanÄ±labilir hale getirmek.


### Bu ayrÄ±m sayesinde:

Veri kalitesi kontrol edilir

Her adÄ±mda log tutulabilir

Yeniden iÅŸleme kolaylaÅŸÄ±r



## ğŸ’¾ Delta Lake ve Delta Tablolar

**Delta Lake**, Apache Spark tabanlÄ± bir "veri gÃ¶lÃ¼" (data lake) formatÄ±dÄ±r. AvantajlarÄ±:

ACID iÅŸlemleri destekler (gÃ¼venli veri iÅŸleme)

Versiyonlama saÄŸlar (time travel)

Efficient upsert/merge iÅŸlemleri

Schema enforcement & evolution destekler

**Delta Table** Ne Zaman KullanÄ±lÄ±r?

Veri gÃ¼ncellenebilir, deÄŸiÅŸebilir durumdaysa (Silver ve Gold katmanlar)

Veri analizi ve raporlama yapÄ±lacaksa

Incremental iÅŸlem yapÄ±lacaksa (Ã¶rn. sadece yeni gelen veriyi ekleme


ğŸ› ï¸ Tablo OluÅŸturma SÃ¼reci ve DataFrame KullanÄ±mÄ±

ğŸ’¡ DataFrame Nedir?

PySpark veya Fabric notebook'larÄ±nda veri Ã¼zerinde iÅŸlem yapmak iÃ§in kullanÄ±lan ana yapÄ±dÄ±r.

SQL gibi dÃ¼ÅŸÃ¼nebilirsin ama Python Ã¼zerinden veri ile Ã§alÄ±ÅŸmanÄ± saÄŸlar.

## âš™ï¸ Delta Tablo OluÅŸturma AÅŸamalarÄ±:

### DataFrame oluÅŸtur:
```
from pyspark.sql.functions import *
df = spark.read.format("csv").option("header", True).load("/path/to/file.csv")
```

### Temizlik, dÃ¶nÃ¼ÅŸÃ¼m iÅŸlemleri:
```
df_clean = df.withColumn("date", to_date("date", "yyyy-MM-dd"))
```

### Delta tablo olarak kaydet:
```
df_clean.write.format("delta").mode("overwrite").saveAsTable("bronze.orders")
```
**Not:** Delta tabloyu doÄŸrudan CREATE TABLE SQL komutuyla da oluÅŸturabilirsin ama genelde ilk DataFrame ile baÅŸlamak daha pratiktir.

# ğŸ§© Temel Kavramlar ve Ne Zaman KullanÄ±lÄ±r?
Kavram	Ne iÅŸe yarar?	Ne zaman kullanÄ±lÄ±r?
#### Delta tablo	Veri Ã¼zerinde versiyonlama, gÃ¼ncelleme, silme, sorgulama saÄŸlar.	TÃ¼m katmanlarda veri tutmak iÃ§in.
#### DataFrame	Bellekte geÃ§ici veri yapÄ±sÄ±dÄ±r.	Veriyi iÅŸlerken, dÃ¶nÃ¼ÅŸtÃ¼rÃ¼rken veya tabloya yazmadan Ã¶nce.
#### Dimension tablo	Referans verisi: mÃ¼ÅŸteri, Ã¼rÃ¼n, tarih gibi.	Gold katmanda.
#### Fact tablo	Ã–lÃ§Ã¼m verileri: satÄ±ÅŸlar, miktarlar vb.	Gold katmanda.
#### Schema / SÃ¼tun tipi belirleme	Veriyi kontrol altÄ±nda tutmak iÃ§in.	Veri yazarken, okurken.
#### CREATE TABLE / MERGE INTO	Delta tablo oluÅŸturur ya da gÃ¼nceller.	BaÅŸlangÄ±Ã§ta veya veri gÃ¼ncellemede.

# ğŸ”§ AdÄ±m AdÄ±m SÃ¼reÃ§ ve Kod AÃ§Ä±klamalarÄ±
## ğŸ¥‰ 1. Bronze Katman â€“ Ham Veriyi Delta FormatÄ±nda Kaydet
ğŸ“Œ AmaÃ§:
Ham veriyi bozmadan saklamak. Gelecekte sorun Ã§Ä±karsa geri dÃ¶nebilmek.

**Format:** CSV, JSON, Parquet

### Ä°ÅŸlemler:

**Kaynaktan veri okuma**

**Temel schema belirleme**

**Ham veri saklama (hiÃ§bir temizlik yok)**


### ğŸ“ AdÄ±mlar:
```
python

bronze_df = spark.read.csv('/path/to/raw.csv', header=True, inferSchema=True)
```
#### âœ… DataFrame oluÅŸturduk. Ã‡Ã¼nkÃ¼ gelen veriyi iÅŸlememiz gerekiyor.

```
python
bronze_df.write.format("delta").mode("overwrite").save("/lakehouse/bronze/raw_table")
```
#### âœ… Delta tabloya yazÄ±yoruz. Bu bir fiziksel tablodur ama henÃ¼z metastoreâ€™da tanÄ±mlÄ± deÄŸil.

```
sql
-- Spark SQL ile tabloyu gÃ¶rÃ¼nÃ¼r yapmak
CREATE TABLE IF NOT EXISTS bronze_raw
USING DELTA
LOCATION '/lakehouse/bronze/raw_table';
```
âœ… CREATE TABLE komutu ile artÄ±k Fabric veya Not Defteri iÃ§inden bu tabloyu sorgulayabiliriz.


## ğŸ¥ˆ 2. Silver Katman â€“ Temizleme, DÃ¶nÃ¼ÅŸtÃ¼rme
ğŸ“Œ AmaÃ§:
Eksik veriyi sil, gereksiz kolonlarÄ± at tarihleri dÃ¼zenle, bozuk kayÄ±tlarÄ± Ã§Ä±kar veri tÃ¼rlerini dÃ¶nÃ¼ÅŸtÃ¼r, tipleri dÃ¼zelt.Null deÄŸerleri temizle ArtÄ±k analiz yapÄ±labilir bir veri elde et.


```
python
silver_df = bronze_df.dropna() \
    .withColumn("tarih", to_date(col("tarih"), "yyyy-MM-dd"))
```
#### âœ… dropna() ile boÅŸ kayÄ±tlar gitti.

#### âœ… withColumn ile tarih formatÄ± dÃ¼zeldi. Åimdi veri â€œtemizâ€.

```
python
silver_df.write.format("delta").mode("overwrite").save("/lakehouse/silver/clean_table")
```
#### âœ… TemizlenmiÅŸ veriyi yeni bir Delta tabloya kaydettik.

```
sql
CREATE TABLE IF NOT EXISTS silver_clean
USING DELTA
LOCATION '/lakehouse/silver/clean_table';
```


## ğŸ¥‡ 3. Gold Katman â€“ Dimension & Fact Tablolar
ğŸ“Œ AmaÃ§:
Power BI raporlarÄ± iÃ§in uygun modeller oluÅŸturmak.

### Ä°ÅŸlemler:

**Join iÅŸlemleri (fact + dimension)**

**Hesaplamalar (Ã¶rn. total sales)**

**Business logiÄŸe uygun modelleme**

### ğŸ¯ Dimension Tablo Ã–rneÄŸi:
```
python
dim_customer_df = silver_df.select("customer_id", "customer_name").dropDuplicates()
```

#### âœ… select ve dropDuplicates ile boyut tablosunu oluÅŸturduk.

```
python
dim_customer_df.write.format("delta").mode("overwrite").save("/lakehouse/gold/dim_customer")
```
```
sql
CREATE TABLE IF NOT EXISTS dim_customer
USING DELTA
LOCATION '/lakehouse/gold/dim_customer';
```

### Fact ve Dimension Tablolar

#### Dimension

Sabit bilgiler (mÃ¼ÅŸteri, Ã¼rÃ¼n)

#### Fact

Olay/veri (sipariÅŸ, satÄ±ÅŸ)

### Ne Zaman KullanÄ±lÄ±r?

Gold katmanda veri modelleme yaparken

Power BI gibi araÃ§lara baÄŸlanmadan Ã¶nce



### ğŸ“Š Fact Tablo Ã–rneÄŸi:

```
python
fact_sales_df = silver_df.groupBy("customer_id").agg(sum("sales").alias("total_sales"))
```
#### âœ… Analize uygun Ã¶lÃ§Ã¼m (fact) verisi: toplam satÄ±ÅŸlar.

```
python
fact_sales_df.write.format("delta").mode("overwrite").save("/lakehouse/gold/fact_sales")
```

```
sql
CREATE TABLE IF NOT EXISTS fact_sales
USING DELTA
LOCATION '/lakehouse/gold/fact_sales';
```

### ğŸ” Delta GÃ¼ncelleme (Opsiyonel)
EÄŸer tabloya her seferinde veri eklemek yerine â€œgÃ¼ncellemeâ€ yapmak istiyorsan:

```
python
from delta.tables import DeltaTable

deltaTable = DeltaTable.forPath(spark, "/lakehouse/silver/clean_table")

deltaTable.alias("target").merge(
    source=silver_df.alias("source"),
    condition="target.id = source.id"
).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()
```

#### âœ… Bu kodla mevcut tabloya yeni veriler eklenir veya var olanlar gÃ¼ncellenir.


## ğŸ§  Zihinsel Yol HaritasÄ±: â€œNe zaman ne yapmalÄ±yÄ±m?â€
## SÃ¼reÃ§	Ne yapÄ±lmalÄ±?	Hangi araÃ§/kod?

**Veri geldiÄŸinde	DataFrame ile oku	spark.read.**()

**Ham saklama	Delta tabloya yaz	.write.format("delta")...**

**GÃ¶rÃ¼nÃ¼r hale getirme	SQL ile tabloyu CREATE ET	CREATE TABLE ... LOCATION ...**

**Veri temizleme	DataFrame iÅŸlemleri (dropna, withColumn)**	
**df.dropna().withColumn(...)**

**Temiz veri saklama	Yeni Delta tabloya yaz	.write.format("delta")...**

**Rapor iÃ§in hazÄ±rlÄ±k	Dimension & Fact tablolar oluÅŸtur	select, groupBy, agg**

**GÃ¼ncelleme gerekiyorsa	MERGE INTO iÅŸlemi	DeltaTable.merge(...)**

## ğŸ§© Ekstra Bilgiler
### DataFrame:
 Kod iÃ§inde veriyi iÅŸlemenin temel yapÄ±sÄ±dÄ±r. Tablolara veri yazmadan Ã¶nce her zaman bir DataFrame Ã¼zerinde Ã§alÄ±ÅŸÄ±rÄ±z.

### Delta tablo:
 KalÄ±cÄ±, versiyonlu ve sorgulanabilir veri yapÄ±sÄ±dÄ±r. Ã–zellikle â€œveri ambarÄ±â€ kurarken bu format tercih edilir.

### SQL ile CREATE TABLE:
 Tabloyu metastoreâ€™a kaydederek gÃ¶rsel araÃ§lardan ulaÅŸÄ±labilir hale getiririz (Ã¶rn. Power BI).

### DeltaTable.merge: 
GÃ¼ncelleme (upsert) yapar. Her veri yeniden yazÄ±lmak zorunda kalmaz.