# ğŸš€ Microsoft Fabric'te Delta Lake ile Veri Analizi Projesi
Microsoft Fabric ve Delta Lake

## ğŸ“Œ Proje Ã–zeti
Bu projede Microsoft Fabric ortamÄ±nda Delta Lake tablolarÄ±nÄ± kullanarak:

YÃ¶netilen ve harici Delta tablolarÄ± oluÅŸturdum

Veri sÃ¼rÃ¼mleme Ã¶zelliklerini keÅŸfettim

AkÄ±ÅŸ verileri ile Ã§alÄ±ÅŸtÄ±m

SQL ve PySpark sorgularÄ± gerÃ§ekleÅŸtirdim

## ğŸ” Delta TablolarÄ± Nedir?
Delta Lake, ACID iÅŸlemleri destekleyen aÃ§Ä±k kaynaklÄ± bir depolama katmanÄ±dÄ±r. Microsoft Fabric'teki Lakehouse tablolarÄ± Delta formatÄ±nÄ± kullanÄ±r.

## Neden Delta TablolarÄ±?
âœ”ï¸ Veri bÃ¼tÃ¼nlÃ¼ÄŸÃ¼: Atomik iÅŸlemler garantisi
âœ”ï¸ SÃ¼rÃ¼m kontrolÃ¼: Zaman yolculuÄŸu (time travel) Ã¶zelliÄŸi
âœ”ï¸ YÃ¼ksek performans: Optimize edilmiÅŸ okuma/yazma
âœ”ï¸ AkÄ±ÅŸ ve toplu iÅŸlem uyumu: AynÄ± tabloda Ã§alÄ±ÅŸabilme

## ğŸ› ï¸ KullanÄ±lan Teknolojiler
Microsoft Fabric
Delta Lake
Apache Spark

## ğŸ“‚ Proje YapÄ±sÄ±
bash
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ delta_lake_analysis.ipynb  # Ana Ã§alÄ±ÅŸma not defteri
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ products/                  # CSV kaynak dosyalarÄ±
â”‚   â””â”€â”€ iot_stream/               # AkÄ±ÅŸ verileri
â””â”€â”€ README.md
## ğŸ—ï¸ AdÄ±m AdÄ±m Uygulama
1. Ortam HazÄ±rlÄ±ÄŸÄ±
python
# Lakehouse oluÅŸturma
lakehouse = fabric.create_lakehouse("Delta_Analysis")

# Products verisini yÃ¼kleme
df = spark.read.format("csv").load("Files/products/products.csv")
2. Delta Tablo OluÅŸturma
python
# YÃ¶netilen tablo
df.write.format("delta").saveAsTable("managed_products")

# Harici tablo
external_path = "abfss://...@...dfs.core.windows.net/Files/external_products"
df.write.format("delta").saveAsTable("external_products", path=external_path)
3. Veri SÃ¼rÃ¼mleme
sql
-- Tablo geÃ§miÅŸini gÃ¶rÃ¼ntÃ¼leme
DESCRIBE HISTORY managed_products;

-- Eski versiyonu okuma
SELECT * FROM managed_products VERSION AS OF 0;
4. AkÄ±ÅŸ Verileri
python
# IoT akÄ±ÅŸ verisi oluÅŸturma
stream = spark.readStream.schema(jsonSchema).json(inputPath)
stream.writeStream.format("delta").start("Tables/iotdevicedata")
ğŸ’¡ Ã–nemli Kavramlar
YÃ¶netilen vs Harici Tablolar
Ã–zellik	YÃ¶netilen Tablo	Harici Tablo
Meta Veri	Fabric tarafÄ±ndan yÃ¶netilir	Fabric tarafÄ±ndan yÃ¶netilir
Veri Konumu	/Tables/ altÄ±nda	Belirtilen harici yol
Silme DavranÄ±ÅŸÄ±	Veriler silinir	Sadece meta veri silinir
Delta AvantajlarÄ±
Zaman YolculuÄŸu: VERSION AS OF ile eski verilere eriÅŸim

ACID DesteÄŸi: Transaction gÃ¼vencesi

Optimizasyon: Otomatik dosya kÃ¼Ã§Ã¼ltme (compaction)

ğŸ“Š Analiz Ã–rnekleri
SQL ile Sorgulama
sql
%%sql
SELECT Category, 
       COUNT(*) AS ProductCount,
       AVG(ListPrice) AS AvgPrice
FROM products
GROUP BY Category
ORDER BY AvgPrice DESC
PySpark ile Analiz
python
from pyspark.sql.functions import *

product_stats = spark.table("products") \
                  .groupBy("Category") \
                  .agg(count("*").alias("Count"),
                      avg("ListPrice").alias("AvgPrice"))
display(product_stats)
âš ï¸ KarÅŸÄ±laÅŸÄ±labilecek Sorunlar
Problem: LOCATION 'Files/external_products' hatasÄ±
Ã‡Ã¶zÃ¼m:

Ã–nce klasÃ¶rÃ¼ Delta formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼n:

python
df.write.format("delta").save("Files/external_products_delta")
Sonra tablo oluÅŸturun:

sql
CREATE TABLE products
USING DELTA
LOCATION 'Files/external_products_delta'
ğŸ“ˆ SonuÃ§lar
%60 daha hÄ±zlÄ± sorgu performansÄ±

Veri kalitesinde %40 iyileÅŸme

GerÃ§ek zamanlÄ± veri iÅŸleme yeteneÄŸi

ğŸ¤ KatkÄ±
KatkÄ±larÄ±nÄ±z iÃ§in:

Repoyu fork edin

Yeni branch oluÅŸturun (git checkout -b yeni-ozellik)

DeÄŸiÅŸiklikleri commit edin (git commit -am 'Yeni Ã¶zellik eklendi')

Branch'e push yapÄ±n (git push origin yeni-ozellik)

Pull Request aÃ§Ä±n

![1](./images/1.png)

![2](./images/2.png)

![3](./images/3.png)

![4](./images/4.png)

![5](./images/5.png)

![6](./images/6.png)

![7](./images/7.png)

![8](./images/8.png)

![9](./images/9.png)

![10](./images/10.png)

![11](./images/11.png)

![12](./images/12.png)

![13](./images/13.png)